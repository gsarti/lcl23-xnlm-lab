{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Z6G_DJNyYXAA8KMnOzle7OmC_nS-N_p5?usp=sharing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assessing MLM Transformer Model Syntactic Abilities**\n",
    "\n",
    "In this notebook, we will see how to assess the syntactic abilities of a Transformer model trained with Masked Language Modeling (MLM) objective function. In particular, we will test the abilities of the model on the *subject-verb agreement* phenomena.\n",
    "\n",
    "The notebook is adapted from the experiments made by Yoav Goldberg in \"*Assessing BERT's Syntactic Abilities*\" (https://arxiv.org/pdf/1901.05287.pdf).\n",
    "For further details, please also see the original github repo by Yoav Goldberg: https://github.com/yoavg/bert-syntax. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Masked Language Modeling**\n",
    "\n",
    "BERT is trained to approximate the Masked Language Modeling function, i.e. predict the identity of masked words in an input sequence (e.g. sentence).\n",
    "\n",
    "<div>\n",
    "  <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200422002516/maskedLanguage.jpg\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "(Source: https://www.geeksforgeeks.org/understanding-bert-nlp/)\n",
    "\n",
    "Relying on the MLM training function, wee can test the performance of the model in learning subject-verb agreement patterns.\n",
    "From Goldberg's paper: *‚Äúfeeding into BERT the complete sentence, while masking out the single focus verb. I then ask BERT for its word predictions for the masked position, and compare the score assigned to the original correct verb to the score assigned to the incorrect one.‚Äù*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Installation and imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Dataset**\n",
    "\n",
    "Below is reported how to load and format the dataset so that it can then be passed directly to the Transformer model. \n",
    "\n",
    "In this notebook, we will be utilizing the dataset defined in \"Targeted Syntactic Evaluation of Language Models\" (Marvin and Linzen, 2018, link to the paper: https://aclanthology.org/D18-1151.pdf). \n",
    "The dataset consists of various stimuli designed to assess the language model's proficiency in the following syntactic phenomena:\n",
    "\n",
    "\n",
    "*   Subject-Verb Agreement;\n",
    "*   Reflexive Anaphora;\n",
    "*   Negative Polarity Items.\n",
    "\n",
    "The dataset can be download from https://github.com/yoavg/bert-syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Google Drive folder\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Data Investigation**\n",
    "\n",
    "Before proceeding with the data processing, let's examine the structure of the dataset more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = \"/content/drive/My Drive/Lectures_2023/marvin_linzen_dataset.tsv\"\n",
    "\n",
    "df = pd.read_csv(test_dataset, delimiter='\\t', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the different typologies of phenomena present in the dataset\n",
    "df[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples from the dataset with simple subject-verb agreement\n",
    "df[df[0] == 'simple_agrmt'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples from the dataset with subject-verb agreement with Long VP (verb phrase) coordination\n",
    "df[df[0] == 'long_vp_coord'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Data Preparation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code are adapted from the script *eval_bert.py* available here: https://github.com/yoavg/bert-syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "\n",
    "cc = Counter()\n",
    "for line in open(test_dataset, 'r'):\n",
    "  sample = line.strip().split(\"\\t\")\n",
    "\n",
    "  # Select only the configuration with simple subject-verb agreement\n",
    "  if sample[0] == \"simple_agrmt\":\n",
    "    cc[sample[1]]+=1\n",
    "\n",
    "    # Select the correct ('g') and the erroneous sentence ('ug)\n",
    "    g, ug = sample[-2], sample[-1]\n",
    "    g = g.split()\n",
    "    ug = ug.split()\n",
    "    assert(len(g)==len(ug)),(g,ug)\n",
    "\n",
    "    # Identify the difference between the two sentences (i.e. the different token)\n",
    "    diffs = [i for i,pair in enumerate(zip(g,ug)) if pair[0]!=pair[1]]\n",
    "    if (len(diffs)!=1):\n",
    "      continue    \n",
    "    assert(len(diffs)==1),diffs\n",
    "\n",
    "    # Save in 'gv' and 'ugv' the correct and incorrect token\n",
    "    gv=g[diffs[0]]   # correct\n",
    "    ugv=ug[diffs[0]] # incorrect\n",
    "\n",
    "    # Recreate the input sequence by replace the target token with [MASK]\n",
    "    g[diffs[0]]=\"[MASK]\"\n",
    "    g.append(\".\")\n",
    "\n",
    "    # Filter the sentences that contains 'swims' as possible target token, since 'swims' does not exist in the model vocabulary\n",
    "    if gv != 'swims' and ugv != 'swims':\n",
    "      processed_dataset.append((sample[0],sample[1],\" \".join(g),gv,ugv))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the data preparation process, each instance will have the following structure:\n",
    "\n",
    "*   Phenomena;\n",
    "*   Construction Template;\n",
    "*   Sentence with the masked target token;\n",
    "*   Target token with correct agreement;\n",
    "*    Target token with incorrect agreement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples:\", len(processed_dataset))\n",
    "print()\n",
    "\n",
    "print(\"Input sample:\", processed_dataset[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Loading the Pipeline**\n",
    "\n",
    "To carry out the experiments, we rely on the Huggingface's Transformers Library. \n",
    "\n",
    "Transformers ü§ó (https://huggingface.co/docs/transformers/index) ‚Äú*provides APIs and tools to easily download and train state-of-the-art pretrained models.*‚Äù\n",
    "\n",
    "For our specific scenario, we utilize the *pipeline* object (https://huggingface.co/docs/transformers/main_classes/pipelines), which offers a simple API dedicated to several tasks (e.g. Named Entity Recognition, Masked Language Modeling, Feature Extraction, and more).\n",
    "\n",
    "The *pipeline()* object can be instantiated as follows:\n",
    "\n",
    "```python\n",
    "nlp = pipeline(<task_name>, model=<model_name>)\n",
    "```\n",
    "\n",
    "In this notebook, we use *bert-base-cased* as Transformer model (12 layers, 12 attention heads, 768 hidden units). However, it is possible to use any other model that has been trained with the MLM function. The full list of available models can be found at the following link: [Huggingface Models](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"fill-mask\", model=\"bert-base-cased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Running the pipeline on the dataset**\n",
    "\n",
    "After loading the dataset and the model, we can simple call the *pipeline* on one item (i.e. sentence) as follows:\n",
    "\n",
    "```python\n",
    "predictions = nlp(<sentence>, targets=<target_tokens>)\n",
    "```\n",
    "\n",
    "\n",
    "The *targets* parameters allows us to provide the model with a set of target tokens in order to compute their probability for the MLM task.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select a sample sentence (and the corresponding target tokens) from our dataset\n",
    "sentence = processed_dataset[10][2]\n",
    "targets = processed_dataset[10][3:]\n",
    "\n",
    "# Sample sentence and target tokens\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Targets:\", targets)\n",
    "print()\n",
    "\n",
    "predictions = nlp(sentence, targets=targets)\n",
    "\n",
    "# Model MLM predictions\n",
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we apply our pipeline to all the sentences in the dataset and then we store the results in a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"phenomena\", \"template\", \"sentence\", \n",
    "           \"correct_token\", \"prob_correct_token\", \n",
    "           \"incorrect_token\", \"prob_incorrect_token\"]\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for input_sample in processed_dataset:\n",
    "  sentence = input_sample[2]\n",
    "  targets = input_sample[3:]\n",
    "\n",
    "  dict_results = {\"phenomena\": [input_sample[0]],\n",
    "                  \"template\": [input_sample[1]],\n",
    "                  \"sentence\": [sentence],\n",
    "                  \"correct_token\": [targets[0]],\n",
    "                  \"incorrect_token\": [targets[1]]}\n",
    "  \n",
    "  # Get BERT predictions and store in the  dictionary\n",
    "  predictions = nlp(sentence, targets=targets)\n",
    "  for pred in predictions:\n",
    "    token = pred[\"token_str\"]\n",
    "    prob = pred[\"score\"]\n",
    "    if token == targets[0]:\n",
    "      dict_results[\"prob_correct_token\"] = [prob]\n",
    "    else:\n",
    "      dict_results[\"prob_incorrect_token\"] = [prob]\n",
    "\n",
    "  df = pd.concat([df, pd.DataFrame(dict_results)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 10 rows of the resulting Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Evaluation**\n",
    "\n",
    "As a final step, we compute the performance of the model. Specifically, we calculate its accuracy, i.e. we verify how many times BERT was able to assign a higher probability to the target word with the correct subject-verb agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "corr = df.query('prob_correct_token > prob_incorrect_token')\n",
    "miss = df.query('prob_correct_token < prob_incorrect_token')\n",
    "\n",
    "tot_corr = len(corr)\n",
    "tot_miss = len(miss)\n",
    "\n",
    "print(\"Total number of correct predictions:\", tot_corr)\n",
    "print(\"Total number of wrong predictions:\", tot_miss)\n",
    "print()\n",
    "\n",
    "accuracy = tot_corr/(tot_corr+tot_miss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct predictions\n",
    "corr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect predictions\n",
    "miss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
