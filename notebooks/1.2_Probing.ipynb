{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Probing Transformer Internal Representations**\n","\n","In this notebook, we will see how to train a probing model to assess the amount of linguistic knoweldge encoded in a pre-trained version of a Transformer model. More specifically, we will test whether and how sentence-level morpho-syntactic and syntatic properites are implicitly encoded in the internal representations of a BERT model.\n","\n","This notebook is adapted from the experiments devised in \"*Linguistic Profiling of a Neural Language Model*\" (Miaschi et al., 2020, https://aclanthology.org/2020.coling-main.65.pdf) and is structured as follows:\n","\n","*  **Part 1**: Loading of the model and the dataset we will use to perform the probing experiments;\n","* **Part 2**: Extraction of the internal representations from the model;\n","* **Part 3**: Training of the probing model;\n","* **Part 4**: Evaluation and visualization of the results."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **Probing Tasks**\n","\n","“*We take a model that was trained on some task, such as a language model. We generate representations using the model, and train another classifier that takes the representations and predicts some property. If the classifier performs well, we say that the model has learned information relevant for the property.*” (Probing Classifiers: Promises, Shortcomings, and Advances, Belinkov Y., 2022, https://direct.mit.edu/coli/article/48/1/207/107571/Probing-Classifiers-Promises-Shortcomings-and).\n","\n","<div>\n","  <img src=\"https://alemiaschi.github.io/files/probing_tasks.png\" width=\"1000\">\n","</div>\n","\n","In this tutorial, we will be focusing on training a probing model that utilizes a linear regressor. The model leverages BERT sentence-level representations to predict the value of a set of linguistic features automatically extracted from gold annotated sentences."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **1. Installation and imports**"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["import os\n","from google.colab import drive\n","\n","import pickle\n","\n","import torch\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# AutoModel and AutoTokenizer for loading the Transformer model (and the corresponding tokenizer) via Huggingface's Transformers library\n","from transformers import AutoModel, AutoTokenizer\n","\n","# Imports for the probing model\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import LinearSVR\n","from sklearn.dummy import DummyRegressor\n","\n","from sklearn.metrics import r2_score, mean_squared_error\n","from scipy.stats import spearmanr"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **1. Loading the model**\n","\n","We will use a pre-trained version of the BERT model (12 layers, 12 attention heads, 768 hidden units), available through the [Transformers](https://huggingface.co/docs/transformers/index) library. However, it is possible to use any other model from those available from the Huggingface Model: [Huggingface Models](https://huggingface.co/models).\n","\n","Along with the model, we also load the corresponding tokenizer.\n","\n","### **1.1 BERT architecture**\n","\n","<div>\n","  <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200420231335/elmo-eemmbeddings.jpg\" width=\"800\">\n","</div>\n","\n","(Source: https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["model_name = \"bert-base-uncased\"\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Loading the tokenizer and the model via Huggingface's Transformers library\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# We set 'output_hidden_states' equal to 'True' because we want to extract the representations from all the layers of the model\n","model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n","model = model.to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **2. Dataset**\n","\n","In this notebook, we employ a dataset comprising sentences extracted from the GUM Corpus (Zeldes A., 2017, https://gucorpling.org/amir/pdf/GUM_paper_prepub.pdf). GUM, the Georgetown University Multilayer corpus, is an open source collection of richly annotated texts from multiple text types. \n","\n","We chose this dataset since it has been annotated according to the [Universal Dependencies](https://universaldependencies.org/) formalism. Therefore, it allows the extraction of gold linguistc properites from the annotated sentences.\n","\n","The UD English GUM is available at the following link: https://universaldependencies.org/treebanks/en_gum/index.html."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["drive.mount('/content/drive')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **2.1. Data Investigation**\n","\n","Before proceeding with the data processing, let's examine the structure of the dataset more closely."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["sentences_file = \"/content/drive/My Drive/Lectures_2023/GUM_Treebank_Sentences.tsv\"\n","\n","df = pd.read_csv(sentences_file, delimiter='\\t', header=None)\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["tot_sentences = len(df)\n","print(\"Total number of sentences:\", tot_sentences)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **3. Extraction of the internal representations**\n","\n","After loading the model and the dataset, we can extract BERT's internal representations. First, we need to tokenize each individual sequence (i.e. sentence) using the tokenizer:\n","\n","```python\n","input_ids = tokenizer.encode(<sentence>, add_special_tokens=True)\n","```\n","We set the parameter '*add_special_tokens*' equal to '*True*' because we want to add the '*\\[CLS]*' and '*\\[SEP]*' tokens defined in the model's vocabulary.\n","\n","<div>\n","  <img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ktkqnCsFx1VPmCNYUOUAyw.png\" width=\"800\">\n","</div>\n","\n","(Source: https://towardsdatascience.com/sentence-correctness-classifier-using-transfer-learning-with-huggingface-bert-8884795ba5ca)\n","\n","\n","Then, we can extract the representations as follows:\n","\n","```python\n","outputs = model(input_ids)\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["embeddings = []\n","ids = []\n","sentences = []\n","\n","# Extracting the representations from each sentence of the probing dataset, with model(*input_ids*), where *input_ids* corresponds to the tokenized sentence\n","with open(sentences_file, 'r') as f:\n","  for line in f:\n","    elements = line.rstrip('\\n').split('\\t')\n","    sent_id = elements[0]\n","    sentence = elements[1]\n","\n","    # Tokenization of the sentence\n","    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n","    input_ids = torch.tensor([input_ids]).to(device)\n","\n","    # Extraction of the representations\n","    with torch.no_grad():\n","      outputs = model(input_ids)\n","      embeddings.append(outputs[\"hidden_states\"])\n","      ids.append(sent_id)\n","      sentences.append(sentence)\n","\n","# Saving the representations \n","with open(\"/content/drive/My Drive/Lectures_2023/GUM_Treebank_Representations.pkl\", \"wb\") as fOut:\n","  pickle.dump({'ids': ids, 'sentences': sentences, 'embeddings': embeddings}, fOut)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **3.1. Exploring Model output**\n","\n","In the following, we examine more closely the output of the extraction process (e.g. number of layers, struture of the output).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["sample_sentence = sentences[300]\n","sample_tokens = tokenizer.tokenize(sample_sentence, add_special_tokens=True)\n","sample_embedding = embeddings[300]\n","\n","# Visualize the output of the tokenization process on the sample sentence extracted from the dataset\n","print(\"Tokens:\", sample_tokens)\n","print()\n","\n","# Number of layers of the model\n","n_layers = len(sample_embedding)\n","print(\"Numer of layers:\", n_layers)\n","print()\n","\n","# Word representations of a specific selected layer\n","layer = 12\n","hidden_state = sample_embedding[layer]\n","print(hidden_state)\n","print()\n","\n","# Visualize the dimension of the 'hidden_state' object (i.e. number of sentences * number of tokens * number of hidden units)\n","dim_hidden_state = hidden_state.shape\n","print(\"Tensor dimension:\", dim_hidden_state)\n","print()\n","\n","# Visualize the embedding of a specific token of the selected sample sentence\n","word_embeddings = {k:v for k, v in zip(sample_tokens, hidden_state[0])}\n","print(word_embeddings[\"reason\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **4. Extraction of the linguistic (probing) features**\n","\n","As already mentioned, each probing task will consist in predicting the value of a set of linguistic features automatically extracted from the GUM annotated sentences.\n","\n","The set of features used in the notebook was extracted using ***Profiling-UD*** (Brunato et al., 2020, http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.883.pdf), a text analysis tool inspired to the principles of linguistic profiling that allows the extraction of more than 130 features, spanning across different levels of linguistic annotation. \n","\n","In particular, these features model linguistic phenomena ranging from raw text ones, to morpho–syntactic information and inflectional properties of verbs, to more complex aspects of sentence structure modeling global and local properties of the whole parsed tree and of specific subtrees, such as the order of subjects and objects with respect to the verb, the distribution of UD syntactic relations, and more.\n","\n","<div>\n","  <img src=\"https://alemiaschi.github.io/files/features.png\" width=\"1000\">\n","</div>\n","\n","\n","Profiling-UD is available at the following link: http://linguistic-profiling.italianlp.it/. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **4.1. Loading the dataset with the linguistic features**\n","\n","In the following, we load and explore our dataset, which has already processed with the Profiling-UD tool. However, it is possible to replicate these experiments by using a different set of sentences and uploading them to Profiling-UD in order to extract the linguistic features to probe the Transformer model."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["# Loading the dataset with the linguistic features extracted with ProfilingUD\n","df = pd.read_csv(\"/content/drive/My Drive/Lectures_2023/GUM_Treebank_Features.tsv\", sep='\\t')\n","df = df.set_index(\"identifier\")\n","df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **5. Train the probing model**\n","\n","Below is reported the function for training the probing model. The function takes as input the following arguments:\n","* BERT internal representations;\n","* The dataframe with the linguistic features extracted with Profiling-UD;\n","* The ids of the sentences in the original dataset;\n","* The BERT layer from which extract the representation as input features of the probing model;\n","* The linguistic feature to be used as prediction label of the probing model.\n","\n","**Preprocessing**: Before starting the actual training process, it is necessary to obtain an aggregated representation of each sentence in our dataset using the BERT token-level representations. For instance, we can apply a mean pooling strategy to compute the average of all token representations in each sentence. In our case, to obtain the embeddings representations for our sentence-level tasks we used for each of its 12 layers the activation of the first input token ('*\\[CLS]*'), which is generally used as classification token and somehow summarizes the information from the actual tokens, as suggested in \"*What does BERT learn about the structure of language?*\" (Jawahar et al., 2019, https://aclanthology.org/P19-1356.pdf).\n","\n","**Train/test split**: Next, we split the dataset into training and test sets, using an 80% split for training and a 20% split for the test set (cross-fold validation could also be performed if desired).\n","\n","**Model definition and training**: Finally, we train the probing model. The chosen model for these experiments is a linear Support Vector Regressor (LinearSVR). We opt for a linear model to keep it as simple as possible and avoid interference with the linguistic properties implicitly encoded in the model representations. Since the features to predict are continuous, the model is set up as a regressor.\n","\n","**Evaluation**: Upon completion of the training, we can evaluate the predictions using commonly used regression metrics (e.g., mean squared error (MSE), R-squared (R2) or Spearman correlation). In our case, we use Spearman correlation, but other metrics can also be tested for performance evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["def probing(embeddings, df, ids, feature, layer):\n","\n","  # Dataset containing only the selected linguistic feature\n","  df_feature = df[feature]\n","\n","  X = []\n","  y = []\n","  \n","  for id, sentence in zip(ids, embeddings):\n","    embedding_layer = sentence[layer][0]\n","\n","    # We extract the representation from the [CLS] token, that is the first token in each input sequence, having set 'add_special_tokens' equal to 'True' during tokenization\n","    cls_embedding = embedding_layer[0].tolist()\n","\n","    # Access to the value of the linguistic feature 'feature' for the given sentence, based on the id\n","    feat = df_feature.loc[id]\n","\n","    X.append(cls_embedding)\n","    y.append(feat)\n","\n","  # Diving the dataset into train and test files\n","  X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(X, y, ids, test_size=0.20, random_state=42)\n","\n","  # Defining the probing model\n","  probing_model = LinearSVR(dual=False, loss='squared_epsilon_insensitive')\n","  \n","  # Fit the model on the training data and then predict on X_test\n","  probing_model.fit(X_train, y_train)\n","  y_pred = probing_model.predict(X_test)\n","\n","  # Evaluation of the predictions\n","  corr, p_val = spearmanr(y_test, y_pred)\n","\n","  # Save the predictions for further analysis\n","  df_preds = pd.DataFrame(columns=[\"sent_id\", \n","                                   \"y_true\", \n","                                   \"y_pred\"])\n","  df_preds[\"sent_id\"] = ids_test\n","  df_preds[\"y_true\"] = y_test\n","  df_preds[\"y_pred\"] = y_pred\n","  df_preds.to_csv(f'/content/drive/My Drive/Lectures_2023/probing_task_predictions/results_{layer}_{feature}.tsv', \n","                  sep='\\t', index=True)\n","  \n","  return corr, p_val"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **5.1. Baseline model**\n","\n","To ensure a meaningul comparison of the results, we define a baseline model (LinearSVR) that exclusively relies on the length of the input sentences to predict the set of linguistic features. This baseline serves as a reference point for evaluating the performance of the probing models trained with BERT internal representations."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["def baseline(df, feature):\n","  # Select \"n_tokens\" as X for the baseline probing model\n","  X = df[\"n_tokens\"].to_numpy()\n","  X = X.reshape(-1, 1)\n","  y = df[feature].to_numpy()\n","\n","  # Diving the dataset into train and test files\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n","  \n","  # Defining the probing model\n","  probing_model = LinearSVR(dual=False, loss='squared_epsilon_insensitive')\n","  \n","  # Fit the model on the training data and then predict on X_test\n","  probing_model.fit(X_train, y_train)\n","  y_pred = probing_model.predict(X_test)\n","\n","  # Evaluation of the predictions\n","  corr, p_val = spearmanr(y_test, y_pred)\n","  \n","  return corr, p_val"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Once the two functions for training the probing model and the baseline have been defined, we can execute the probing tasks, one for each layer of the model and for each lingusitic feature extracted with Profiling-UD.\n","\n","Below is the code to perform the probing tasks on all 12 layers of BERT and on a set of 6 linguistic features extracted with Profiling-UD:\n","* *n_tokens*: total number of tokens;\n","* *char_per_tok*: average number of characters per word (excluded punctuation);\n","* *upos_dist_NOUN*: distribution of nouns in the sentence;\n","* *upos_dist_VERB*: distribution of verbs in the sentence;\n","* *avg_token_per_clause*: average clause length, calculated in terms of the average number of tokens per clause, where a clause is defined as the ratio between the number of tokens in a sentence and the number of either verbal or copular head; \n","* *avg_subordinate_chain_len*: average length of subordinate chains, where a subordinate 'chain' is calculated as the number of subordinate clauses embedded on a first subordinate clause.\n","\n","Feel free to customize the code according to your requirements, such as executing it selectively on specific layers, distinct groups of features, or even on all of them."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["selected_features = [\"n_tokens\", \n","                     \"char_per_tok\", \n","                     \"upos_dist_NOUN\", \n","                     \"upos_dist_VERB\", \n","                     \"avg_token_per_clause\", \n","                     \"avg_subordinate_chain_len\"\n","                     ]\n","\n","layers = list(range(n_layers)[1:])\n","\n","# Create an empty pandas DataFrame to store the probing results\n","results = pd.DataFrame(index=selected_features, columns=layers + [\"Baseline\"])\n","\n","# Iterate over the selected features\n","for feature in selected_features:\n","  # Iterate over the 12 layers of the model\n","  for layer in layers:\n","    corr, p_val = probing(embeddings, df, ids, feature, layer)\n","\n","    # Save the score only if the correlation is statistically significant (p-value < 0.05)\n","    if p_val < 0.05:\n","      results.loc[feature][layer] = corr\n","\n","  # Simple baseline model trained to predict the selected linguistic feature using only \"n_tokens\" as input feature\n","  corr_baseline, p_val_baseline = baseline(df, feature)\n","  if p_val < 0.05:\n","    results.loc[feature][\"Baseline\"] = corr_baseline"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **6. Results visualization**\n","\n","We can now look at the results that we previously stored in a DataFrame and compare with respect to the '*n_tokens*' baseline.\n","\n","Since we have performed the probing tasks using representations extracted from all 12 layers of the model, we can also explore the progression across the layers, from the input (1) to the output one (12)."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["# Visualize the results memorized in the DataFrame\n","results"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["# Formatting the dataframe in order to be visualized as a multiline plot\n","results = results[layers]\n","results = results.T\n","results[\"Layers\"] = results.index\n","plot_results = pd.melt(results, [\"Layers\"])\n","\n","# Visualization with seaborn\n","fig, ax = plt.subplots(figsize=(8, 6))\n","g = sns.lineplot(data=plot_results, x=\"Layers\", y=\"value\", hue=\"variable\")\n","handles, labels = g.get_legend_handles_labels()\n","g.legend(handles=handles, labels=labels, bbox_to_anchor=(0., 1.02, 1., .102),\n","              loc='lower left', ncol=3, mode=\"expand\")"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
