{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/lcl23-xnlm-lab/blob/main/notebooks/2_Attribution_Contrastive.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run in Colab to install local packages\n",
    "%pip install ferret-xai\n",
    "%pip install git+https://github.com/inseq-team/inseq.git\n",
    "%pip install numpy==1.23.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Attribution for NLP\n",
    "\n",
    "*See [Madsen et al. 2022](https://dl.acm.org/doi/10.1145/3546577) for a survey on feature attribution methods for NLP applications.*\n",
    "\n",
    "**Feature attribution methods** leverage the internal information (e.g. gradients, attention) or predictions of a model to quantify the relationship between its inputs and its predictions.\n",
    "\n",
    "Attribution methods produce **importance scores** (or *saliency scores*) for every element of the input, reflecting the importance of every input element in driving the model prediction. These scores are often presented using <span style=\"background:#A85E9E\">highlights</span> (or *attribution maps*) to facilitate comprehension, although recent research showed the risks of textual highlights misinterpretation caused by human cognitive biases ([Jacovi et al. 2022](https://dl.acm.org/doi/abs/10.1145/3531146.3533127), [Jacovi et al. 2023](https://arxiv.org/abs/2305.02679))\n",
    "\n",
    "![Example of attribution map for sentiment analysis](https://dl.acm.org/cms/asset/b4fe400e-9eb7-4f10-ab49-ac6a31a8a5cf/csur-2021-0674-f03.jpg)\n",
    "\n",
    "*Hypothetical example of attribution map for sentiment analysis from [Madsen et al. 2022](https://dl.acm.org/doi/10.1145/3546577). c represents the explained (predicted) class, while y is the correct label for the example.*\n",
    "\n",
    "We can categorize feature attribution approaches in three major families:\n",
    "\n",
    "- **Gradient-based methods** such as [Integrated gradients]() use gradients as a natural source of information to motivate model predictions. Gradients for model parameters computed in relation to a loss function are commonly used during training to update model parameters, since they represent *the magnitude of change needed for a parameter such that the prediction matches the target label*. In the case of feature attribution, gradients computed with respect to a model prediction logit or probability are instead taken as *how much the parameter is contributing towards the prediction*.\n",
    "\n",
    "<img alt=\"How gradient attribution is computed\" src=\"https://jalammar.github.io/images/explaining/111.PNG\" height=300/>\n",
    "\n",
    "*Overview of feature attribution using gradient information. From Jay Alammar blog post \"[Interfaces for Explaining Transformer Language Models](https://jalammar.github.io/explaining-transformers/)\"*\n",
    "\n",
    "- **Perturbation-based methods** such as [Occlusion](https://captum.ai/api/occlusion.html) estimate importance of inputs by introducing noise in the prediction process, usually by masking or removing either input features or network components, and verifying the downstream effect on model predictions. Intuitively, these can also be used to determine the importance of layers in the neural network.\n",
    "\n",
    "- **Internals-based methods** use quantities computed naturally by the network during their predictions to motivate its internal computations. For Transformers-based model, [attention weights](https://aclanthology.org/D19-1002/) are commonly used, by themselves or multiplied with other quantities, as indications of feature importance.\n",
    "\n",
    "<img alt=\"Aligning words across source and target sentences in translation with attention\" src=\"https://jalammar.github.io/images/attention_sentence.png\" height=350/>\n",
    "\n",
    "*Example of attention weights aligning source and target words in a translation task. From Bahdanau et al. 2015 \"[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\"*\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><b>Additional details about feature attribution methods</b></summary>\n",
    "    <ol>\n",
    "        <li> We distinguish gradient and internals methods since gradient methods require a <i>prediction target</i> (e.g. the predicted class label, the next generated word) to compute importance scores, while internals-based one generally do not rely on the predicted output to determine importance.</li>\n",
    "        </br>\n",
    "        <li> Feature importance is generally computed for input tokens, but most methods can also be used to compute importance of intermediate representations (or <i>activations</i>) computed by the model (e.g. stopping at an intermediate layer when backpropagating gradients).</li>\n",
    "        </br>\n",
    "        <li> Methods using layer-specific quantities like attention weights usually aggregate those across layers to obtain a proxy importance for input features, either averaging naively or with more reasonable methods such as <a href=\"https://aclanthology.org/2020.acl-main.385/\">rollout and flow</a>.</li>\n",
    "        </br>\n",
    "        <li>The granularity of importance scores depend on the attribution method that is being used. For example, the attention mechanism operates at a token level, so attention weights used as importance scores will be one per token in the input sequence. Gradients, on the contrary, are computed for all the dimensions of each token embedding, so an aggregation strategy should be used to obtain a single score per token.</li>\n",
    "    </ol>\n",
    "</details>\n",
    "\n",
    "In this lab we will learn how to use modern ü§ó-based tools to compute feature attribution for NLP models and evaluate their results.\n",
    "\n",
    "## Attributing Classification Models with ferret\n",
    "\n",
    "*More info: [ferret Docs](https://ferret.readthedocs.io/en/latest/index.html)*\n",
    "\n",
    "The [ferret](https://github.com/g8a9/ferret) library [(Attanasio et al., 2023)](https://aclanthology.org/2023.eacl-demo.29/) can be used to conveniently attribute the prediction of classification models from the ü§ó Transformers framework. In the following example, a multilingual model finetuned for the sentiment analysis task is loaded, and ferret is used to attribute its predictions using various methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'negative', 1: 'neutral', 2: 'positive'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.8361954092979431}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "print(model.config.id2label)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "nlp(\"Despite your sad haircut, you look truly stunning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_38163_row0_col0 {\n",
       "  background-color: #f1f1f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38163_row0_col1 {\n",
       "  background-color: #eff0f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38163_row0_col2, #T_38163_row0_col9 {\n",
       "  background-color: #f1e4e5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38163_row0_col3 {\n",
       "  background-color: #d5dfe7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38163_row0_col4, #T_38163_row0_col5 {\n",
       "  background-color: #f2eded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38163_row0_col6 {\n",
       "  background-color: #f1eaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38163_row0_col7 {\n",
       "  background-color: #f1e5e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38163_row0_col8 {\n",
       "  background-color: #f0e1e2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38163_row0_col10 {\n",
       "  background-color: #ecbfc2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38163_row0_col11 {\n",
       "  background-color: #f1e8e9;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_38163\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Token</th>\n",
       "      <th id=\"T_38163_level0_col0\" class=\"col_heading level0 col0\" >‚ñÅDe</th>\n",
       "      <th id=\"T_38163_level0_col1\" class=\"col_heading level0 col1\" >spite</th>\n",
       "      <th id=\"T_38163_level0_col2\" class=\"col_heading level0 col2\" >‚ñÅyour</th>\n",
       "      <th id=\"T_38163_level0_col3\" class=\"col_heading level0 col3\" >‚ñÅsad</th>\n",
       "      <th id=\"T_38163_level0_col4\" class=\"col_heading level0 col4\" >‚ñÅhair</th>\n",
       "      <th id=\"T_38163_level0_col5\" class=\"col_heading level0 col5\" >cut</th>\n",
       "      <th id=\"T_38163_level0_col6\" class=\"col_heading level0 col6\" >,</th>\n",
       "      <th id=\"T_38163_level0_col7\" class=\"col_heading level0 col7\" >‚ñÅyou</th>\n",
       "      <th id=\"T_38163_level0_col8\" class=\"col_heading level0 col8\" >‚ñÅlook</th>\n",
       "      <th id=\"T_38163_level0_col9\" class=\"col_heading level0 col9\" >‚ñÅtruly</th>\n",
       "      <th id=\"T_38163_level0_col10\" class=\"col_heading level0 col10\" >‚ñÅstunning</th>\n",
       "      <th id=\"T_38163_level0_col11\" class=\"col_heading level0 col11\" >!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_38163_level0_row0\" class=\"row_heading level0 row0\" >Partition SHAP</th>\n",
       "      <td id=\"T_38163_row0_col0\" class=\"data row0 col0\" >-0.00</td>\n",
       "      <td id=\"T_38163_row0_col1\" class=\"data row0 col1\" >-0.01</td>\n",
       "      <td id=\"T_38163_row0_col2\" class=\"data row0 col2\" >0.07</td>\n",
       "      <td id=\"T_38163_row0_col3\" class=\"data row0 col3\" >-0.16</td>\n",
       "      <td id=\"T_38163_row0_col4\" class=\"data row0 col4\" >0.02</td>\n",
       "      <td id=\"T_38163_row0_col5\" class=\"data row0 col5\" >0.03</td>\n",
       "      <td id=\"T_38163_row0_col6\" class=\"data row0 col6\" >0.04</td>\n",
       "      <td id=\"T_38163_row0_col7\" class=\"data row0 col7\" >0.07</td>\n",
       "      <td id=\"T_38163_row0_col8\" class=\"data row0 col8\" >0.09</td>\n",
       "      <td id=\"T_38163_row0_col9\" class=\"data row0 col9\" >0.07</td>\n",
       "      <td id=\"T_38163_row0_col10\" class=\"data row0 col10\" >0.28</td>\n",
       "      <td id=\"T_38163_row0_col11\" class=\"data row0 col11\" >0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1838636340>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ferret import SHAPExplainer, Benchmark\n",
    "\n",
    "# Attributing the predicted class (positive)\n",
    "bench = Benchmark(model, tokenizer)\n",
    "shap_exp = SHAPExplainer(model, tokenizer)\n",
    "explanations = shap_exp(\"Despite your sad haircut, you look truly stunning!\", target=2)\n",
    "bench.show_table([explanations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0c143_row0_col0, #T_0c143_row0_col1, #T_0c143_row0_col9 {\n",
       "  background-color: #f1f1f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0c143_row0_col2, #T_0c143_row0_col8 {\n",
       "  background-color: #e7ebee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0c143_row0_col3 {\n",
       "  background-color: #eecfd1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0c143_row0_col4, #T_0c143_row0_col5 {\n",
       "  background-color: #eff0f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0c143_row0_col6 {\n",
       "  background-color: #e9edf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0c143_row0_col7 {\n",
       "  background-color: #ebeef0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0c143_row0_col10 {\n",
       "  background-color: #d3dee6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0c143_row0_col11 {\n",
       "  background-color: #eceef1;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0c143\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Token</th>\n",
       "      <th id=\"T_0c143_level0_col0\" class=\"col_heading level0 col0\" >‚ñÅDe</th>\n",
       "      <th id=\"T_0c143_level0_col1\" class=\"col_heading level0 col1\" >spite</th>\n",
       "      <th id=\"T_0c143_level0_col2\" class=\"col_heading level0 col2\" >‚ñÅyour</th>\n",
       "      <th id=\"T_0c143_level0_col3\" class=\"col_heading level0 col3\" >‚ñÅsad</th>\n",
       "      <th id=\"T_0c143_level0_col4\" class=\"col_heading level0 col4\" >‚ñÅhair</th>\n",
       "      <th id=\"T_0c143_level0_col5\" class=\"col_heading level0 col5\" >cut</th>\n",
       "      <th id=\"T_0c143_level0_col6\" class=\"col_heading level0 col6\" >,</th>\n",
       "      <th id=\"T_0c143_level0_col7\" class=\"col_heading level0 col7\" >‚ñÅyou</th>\n",
       "      <th id=\"T_0c143_level0_col8\" class=\"col_heading level0 col8\" >‚ñÅlook</th>\n",
       "      <th id=\"T_0c143_level0_col9\" class=\"col_heading level0 col9\" >‚ñÅtruly</th>\n",
       "      <th id=\"T_0c143_level0_col10\" class=\"col_heading level0 col10\" >‚ñÅstunning</th>\n",
       "      <th id=\"T_0c143_level0_col11\" class=\"col_heading level0 col11\" >!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0c143_level0_row0\" class=\"row_heading level0 row0\" >Partition SHAP</th>\n",
       "      <td id=\"T_0c143_row0_col0\" class=\"data row0 col0\" >-0.00</td>\n",
       "      <td id=\"T_0c143_row0_col1\" class=\"data row0 col1\" >-0.01</td>\n",
       "      <td id=\"T_0c143_row0_col2\" class=\"data row0 col2\" >-0.06</td>\n",
       "      <td id=\"T_0c143_row0_col3\" class=\"data row0 col3\" >0.19</td>\n",
       "      <td id=\"T_0c143_row0_col4\" class=\"data row0 col4\" >-0.01</td>\n",
       "      <td id=\"T_0c143_row0_col5\" class=\"data row0 col5\" >-0.01</td>\n",
       "      <td id=\"T_0c143_row0_col6\" class=\"data row0 col6\" >-0.04</td>\n",
       "      <td id=\"T_0c143_row0_col7\" class=\"data row0 col7\" >-0.03</td>\n",
       "      <td id=\"T_0c143_row0_col8\" class=\"data row0 col8\" >-0.06</td>\n",
       "      <td id=\"T_0c143_row0_col9\" class=\"data row0 col9\" >-0.01</td>\n",
       "      <td id=\"T_0c143_row0_col10\" class=\"data row0 col10\" >-0.17</td>\n",
       "      <td id=\"T_0c143_row0_col11\" class=\"data row0 col11\" >-0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1838387f70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now attributing the negative class -> positive scores should correspond to negative word\n",
    "explanations = shap_exp(\"Despite your sad haircut, you look truly stunning!\", target=0)\n",
    "bench.show_table([explanations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6566f_row0_col0, #T_6566f_row3_col7, #T_6566f_row4_col5 {\n",
       "  background-color: #f1f1f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row0_col1, #T_6566f_row0_col5, #T_6566f_row0_col9, #T_6566f_row1_col6, #T_6566f_row4_col9, #T_6566f_row5_col2 {\n",
       "  background-color: #eff0f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row0_col2, #T_6566f_row5_col4 {\n",
       "  background-color: #e0e6eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row0_col3 {\n",
       "  background-color: #ebb9bd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row0_col4, #T_6566f_row1_col7 {\n",
       "  background-color: #edeff1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row0_col6 {\n",
       "  background-color: #e5eaee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row0_col7, #T_6566f_row0_col11, #T_6566f_row1_col2 {\n",
       "  background-color: #e8ecef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row0_col8, #T_6566f_row1_col0, #T_6566f_row1_col1 {\n",
       "  background-color: #e1e7ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row0_col10, #T_6566f_row1_col10 {\n",
       "  background-color: #c0d1de;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row1_col3, #T_6566f_row2_col3 {\n",
       "  background-color: #edcacd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row1_col4, #T_6566f_row3_col6, #T_6566f_row4_col4, #T_6566f_row5_col7 {\n",
       "  background-color: #eceef1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row1_col5, #T_6566f_row2_col6, #T_6566f_row2_col11, #T_6566f_row4_col2 {\n",
       "  background-color: #f2eded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row1_col8 {\n",
       "  background-color: #e2e8ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row1_col9, #T_6566f_row3_col4 {\n",
       "  background-color: #ebeef0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row1_col11, #T_6566f_row3_col1 {\n",
       "  background-color: #e4e9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row2_col0, #T_6566f_row2_col7 {\n",
       "  background-color: #f2ebeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row2_col1, #T_6566f_row2_col5, #T_6566f_row2_col10 {\n",
       "  background-color: #f0e1e2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row2_col2, #T_6566f_row3_col8 {\n",
       "  background-color: #f1e8e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row2_col4, #T_6566f_row2_col9, #T_6566f_row4_col6 {\n",
       "  background-color: #f0e3e3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row2_col8, #T_6566f_row4_col7 {\n",
       "  background-color: #f1eaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row3_col0, #T_6566f_row4_col0, #T_6566f_row5_col5 {\n",
       "  background-color: #f2eeee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row3_col2 {\n",
       "  background-color: #f1e7e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row3_col3 {\n",
       "  background-color: #cedae4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row3_col5, #T_6566f_row5_col8 {\n",
       "  background-color: #f2f1f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row3_col9 {\n",
       "  background-color: #dae3e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row3_col10, #T_6566f_row5_col9 {\n",
       "  background-color: #d9e2e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row3_col11 {\n",
       "  background-color: #dee5eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row4_col1 {\n",
       "  background-color: #efd7d9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row4_col3 {\n",
       "  background-color: #eecfd1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row4_col8, #T_6566f_row5_col0 {\n",
       "  background-color: #f2efef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row4_col10 {\n",
       "  background-color: #efdbdd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row4_col11, #T_6566f_row5_col11 {\n",
       "  background-color: #e9edf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row5_col1 {\n",
       "  background-color: #f0ddde;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row5_col3 {\n",
       "  background-color: #ecc2c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row5_col6 {\n",
       "  background-color: #e7ebee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6566f_row5_col10 {\n",
       "  background-color: #cbd8e3;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6566f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Token</th>\n",
       "      <th id=\"T_6566f_level0_col0\" class=\"col_heading level0 col0\" >‚ñÅDe</th>\n",
       "      <th id=\"T_6566f_level0_col1\" class=\"col_heading level0 col1\" >spite</th>\n",
       "      <th id=\"T_6566f_level0_col2\" class=\"col_heading level0 col2\" >‚ñÅyour</th>\n",
       "      <th id=\"T_6566f_level0_col3\" class=\"col_heading level0 col3\" >‚ñÅsad</th>\n",
       "      <th id=\"T_6566f_level0_col4\" class=\"col_heading level0 col4\" >‚ñÅhair</th>\n",
       "      <th id=\"T_6566f_level0_col5\" class=\"col_heading level0 col5\" >cut</th>\n",
       "      <th id=\"T_6566f_level0_col6\" class=\"col_heading level0 col6\" >,</th>\n",
       "      <th id=\"T_6566f_level0_col7\" class=\"col_heading level0 col7\" >‚ñÅyou</th>\n",
       "      <th id=\"T_6566f_level0_col8\" class=\"col_heading level0 col8\" >‚ñÅlook</th>\n",
       "      <th id=\"T_6566f_level0_col9\" class=\"col_heading level0 col9\" >‚ñÅtruly</th>\n",
       "      <th id=\"T_6566f_level0_col10\" class=\"col_heading level0 col10\" >‚ñÅstunning</th>\n",
       "      <th id=\"T_6566f_level0_col11\" class=\"col_heading level0 col11\" >!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6566f_level0_row0\" class=\"row_heading level0 row0\" >Partition SHAP</th>\n",
       "      <td id=\"T_6566f_row0_col0\" class=\"data row0 col0\" >-0.01</td>\n",
       "      <td id=\"T_6566f_row0_col1\" class=\"data row0 col1\" >-0.01</td>\n",
       "      <td id=\"T_6566f_row0_col2\" class=\"data row0 col2\" >-0.10</td>\n",
       "      <td id=\"T_6566f_row0_col3\" class=\"data row0 col3\" >0.31</td>\n",
       "      <td id=\"T_6566f_row0_col4\" class=\"data row0 col4\" >-0.02</td>\n",
       "      <td id=\"T_6566f_row0_col5\" class=\"data row0 col5\" >-0.01</td>\n",
       "      <td id=\"T_6566f_row0_col6\" class=\"data row0 col6\" >-0.07</td>\n",
       "      <td id=\"T_6566f_row0_col7\" class=\"data row0 col7\" >-0.05</td>\n",
       "      <td id=\"T_6566f_row0_col8\" class=\"data row0 col8\" >-0.09</td>\n",
       "      <td id=\"T_6566f_row0_col9\" class=\"data row0 col9\" >-0.01</td>\n",
       "      <td id=\"T_6566f_row0_col10\" class=\"data row0 col10\" >-0.27</td>\n",
       "      <td id=\"T_6566f_row0_col11\" class=\"data row0 col11\" >-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6566f_level0_row1\" class=\"row_heading level0 row1\" >LIME</th>\n",
       "      <td id=\"T_6566f_row1_col0\" class=\"data row1 col0\" >-0.09</td>\n",
       "      <td id=\"T_6566f_row1_col1\" class=\"data row1 col1\" >-0.09</td>\n",
       "      <td id=\"T_6566f_row1_col2\" class=\"data row1 col2\" >-0.05</td>\n",
       "      <td id=\"T_6566f_row1_col3\" class=\"data row1 col3\" >0.22</td>\n",
       "      <td id=\"T_6566f_row1_col4\" class=\"data row1 col4\" >-0.03</td>\n",
       "      <td id=\"T_6566f_row1_col5\" class=\"data row1 col5\" >0.03</td>\n",
       "      <td id=\"T_6566f_row1_col6\" class=\"data row1 col6\" >-0.01</td>\n",
       "      <td id=\"T_6566f_row1_col7\" class=\"data row1 col7\" >-0.02</td>\n",
       "      <td id=\"T_6566f_row1_col8\" class=\"data row1 col8\" >-0.08</td>\n",
       "      <td id=\"T_6566f_row1_col9\" class=\"data row1 col9\" >-0.03</td>\n",
       "      <td id=\"T_6566f_row1_col10\" class=\"data row1 col10\" >-0.28</td>\n",
       "      <td id=\"T_6566f_row1_col11\" class=\"data row1 col11\" >-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6566f_level0_row2\" class=\"row_heading level0 row2\" >Gradient</th>\n",
       "      <td id=\"T_6566f_row2_col0\" class=\"data row2 col0\" >0.03</td>\n",
       "      <td id=\"T_6566f_row2_col1\" class=\"data row2 col1\" >0.09</td>\n",
       "      <td id=\"T_6566f_row2_col2\" class=\"data row2 col2\" >0.05</td>\n",
       "      <td id=\"T_6566f_row2_col3\" class=\"data row2 col3\" >0.21</td>\n",
       "      <td id=\"T_6566f_row2_col4\" class=\"data row2 col4\" >0.08</td>\n",
       "      <td id=\"T_6566f_row2_col5\" class=\"data row2 col5\" >0.09</td>\n",
       "      <td id=\"T_6566f_row2_col6\" class=\"data row2 col6\" >0.02</td>\n",
       "      <td id=\"T_6566f_row2_col7\" class=\"data row2 col7\" >0.03</td>\n",
       "      <td id=\"T_6566f_row2_col8\" class=\"data row2 col8\" >0.04</td>\n",
       "      <td id=\"T_6566f_row2_col9\" class=\"data row2 col9\" >0.08</td>\n",
       "      <td id=\"T_6566f_row2_col10\" class=\"data row2 col10\" >0.09</td>\n",
       "      <td id=\"T_6566f_row2_col11\" class=\"data row2 col11\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6566f_level0_row3\" class=\"row_heading level0 row3\" >Gradient (x Input)</th>\n",
       "      <td id=\"T_6566f_row3_col0\" class=\"data row3 col0\" >0.02</td>\n",
       "      <td id=\"T_6566f_row3_col1\" class=\"data row3 col1\" >-0.07</td>\n",
       "      <td id=\"T_6566f_row3_col2\" class=\"data row3 col2\" >0.06</td>\n",
       "      <td id=\"T_6566f_row3_col3\" class=\"data row3 col3\" >-0.20</td>\n",
       "      <td id=\"T_6566f_row3_col4\" class=\"data row3 col4\" >-0.04</td>\n",
       "      <td id=\"T_6566f_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "      <td id=\"T_6566f_row3_col6\" class=\"data row3 col6\" >-0.03</td>\n",
       "      <td id=\"T_6566f_row3_col7\" class=\"data row3 col7\" >-0.00</td>\n",
       "      <td id=\"T_6566f_row3_col8\" class=\"data row3 col8\" >0.05</td>\n",
       "      <td id=\"T_6566f_row3_col9\" class=\"data row3 col9\" >-0.13</td>\n",
       "      <td id=\"T_6566f_row3_col10\" class=\"data row3 col10\" >-0.14</td>\n",
       "      <td id=\"T_6566f_row3_col11\" class=\"data row3 col11\" >-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6566f_level0_row4\" class=\"row_heading level0 row4\" >Integrated Gradient</th>\n",
       "      <td id=\"T_6566f_row4_col0\" class=\"data row4 col0\" >0.02</td>\n",
       "      <td id=\"T_6566f_row4_col1\" class=\"data row4 col1\" >0.14</td>\n",
       "      <td id=\"T_6566f_row4_col2\" class=\"data row4 col2\" >0.03</td>\n",
       "      <td id=\"T_6566f_row4_col3\" class=\"data row4 col3\" >0.19</td>\n",
       "      <td id=\"T_6566f_row4_col4\" class=\"data row4 col4\" >-0.03</td>\n",
       "      <td id=\"T_6566f_row4_col5\" class=\"data row4 col5\" >-0.00</td>\n",
       "      <td id=\"T_6566f_row4_col6\" class=\"data row4 col6\" >0.08</td>\n",
       "      <td id=\"T_6566f_row4_col7\" class=\"data row4 col7\" >0.04</td>\n",
       "      <td id=\"T_6566f_row4_col8\" class=\"data row4 col8\" >0.02</td>\n",
       "      <td id=\"T_6566f_row4_col9\" class=\"data row4 col9\" >-0.01</td>\n",
       "      <td id=\"T_6566f_row4_col10\" class=\"data row4 col10\" >0.12</td>\n",
       "      <td id=\"T_6566f_row4_col11\" class=\"data row4 col11\" >-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6566f_level0_row5\" class=\"row_heading level0 row5\" >Integrated Gradient (x Input)</th>\n",
       "      <td id=\"T_6566f_row5_col0\" class=\"data row5 col0\" >0.01</td>\n",
       "      <td id=\"T_6566f_row5_col1\" class=\"data row5 col1\" >0.12</td>\n",
       "      <td id=\"T_6566f_row5_col2\" class=\"data row5 col2\" >-0.01</td>\n",
       "      <td id=\"T_6566f_row5_col3\" class=\"data row5 col3\" >0.26</td>\n",
       "      <td id=\"T_6566f_row5_col4\" class=\"data row5 col4\" >-0.10</td>\n",
       "      <td id=\"T_6566f_row5_col5\" class=\"data row5 col5\" >0.02</td>\n",
       "      <td id=\"T_6566f_row5_col6\" class=\"data row5 col6\" >-0.06</td>\n",
       "      <td id=\"T_6566f_row5_col7\" class=\"data row5 col7\" >-0.02</td>\n",
       "      <td id=\"T_6566f_row5_col8\" class=\"data row5 col8\" >0.01</td>\n",
       "      <td id=\"T_6566f_row5_col9\" class=\"data row5 col9\" >-0.13</td>\n",
       "      <td id=\"T_6566f_row5_col10\" class=\"data row5 col10\" >-0.22</td>\n",
       "      <td id=\"T_6566f_row5_col11\" class=\"data row5 col11\" >-0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1838350670>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using bench direcly attributes using all available methods\n",
    "all_explanations = bench.explain(\"Despite your sad haircut, you look truly stunning!\", target=0)\n",
    "bench.show_table(all_explanations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Feature Attribution Methods\n",
    "\n",
    "From the results in the tables above we can see that some methods seem to produce more intuitive results than others. But how can we evaluate the quality of these attributions? Generally the evaluation of feature attribution method is centered around two aspects:\n",
    "\n",
    "- **Faithfulness** to model processing, i.e. if the method correctly identifies feature having a causal influence in model predictions.\n",
    "\n",
    "- **Plausibility** of the attribution, i.e. if the method produces results that match human intuition.\n",
    "\n",
    "There is no guarantee that a method will be faithful and plausible at the same time, and the choice of the evaluation metric will depend on the application. We can estimate some faithfulness and evaluation metrics using the `ferret` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e5f7a_row0_col0, #T_e5f7a_row1_col0, #T_e5f7a_row5_col0 {\n",
       "  background-color: #f1e8e9;\n",
       "  color: #000000;\n",
       "  background-color: #b672b6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row0_col1 {\n",
       "  background-color: #9fbbd0;\n",
       "  color: #000000;\n",
       "  background-color: #9f409f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row0_col2 {\n",
       "  background-color: #efdbdd;\n",
       "  color: #000000;\n",
       "  background-color: #b269b2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row0_col3, #T_e5f7a_row0_col4, #T_e5f7a_row0_col5, #T_e5f7a_row1_col3, #T_e5f7a_row1_col4, #T_e5f7a_row1_col5, #T_e5f7a_row2_col3, #T_e5f7a_row2_col4, #T_e5f7a_row2_col5, #T_e5f7a_row4_col3, #T_e5f7a_row4_col4, #T_e5f7a_row4_col5, #T_e5f7a_row5_col3, #T_e5f7a_row5_col4, #T_e5f7a_row5_col5 {\n",
       "  background-color: #da3b46;\n",
       "  color: #f1f1f1;\n",
       "  background-color: #800080;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row1_col1 {\n",
       "  background-color: #a8c1d3;\n",
       "  color: #000000;\n",
       "  background-color: #a146a1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row1_col2 {\n",
       "  background-color: #e58d94;\n",
       "  color: #f1f1f1;\n",
       "  background-color: #9a379a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row2_col0 {\n",
       "  background-color: #e8ecef;\n",
       "  color: #000000;\n",
       "  background-color: #bc7ebc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row2_col1 {\n",
       "  background-color: #c4d4e0;\n",
       "  color: #000000;\n",
       "  background-color: #ab59ab;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row2_col2 {\n",
       "  background-color: #ecc5c7;\n",
       "  color: #000000;\n",
       "  background-color: #ab5aab;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row3_col0 {\n",
       "  background-color: #eff0f2;\n",
       "  color: #000000;\n",
       "  background-color: #ba79ba;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row3_col1 {\n",
       "  background-color: #d1dce5;\n",
       "  color: #000000;\n",
       "  background-color: #af62af;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row3_col2, #T_e5f7a_row5_col2 {\n",
       "  background-color: #ecc0c3;\n",
       "  color: #000000;\n",
       "  background-color: #aa57aa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row3_col3 {\n",
       "  background-color: #f1eaea;\n",
       "  color: #000000;\n",
       "  background-color: #b773b7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row3_col4, #T_e5f7a_row3_col5, #T_e5f7a_row4_col2 {\n",
       "  background-color: #f2f1f1;\n",
       "  color: #000000;\n",
       "  background-color: #b977b9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row4_col0 {\n",
       "  background-color: #f2efef;\n",
       "  color: #000000;\n",
       "  background-color: #b976b9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row4_col1 {\n",
       "  background-color: #b3c8d8;\n",
       "  color: #000000;\n",
       "  background-color: #a54da5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5f7a_row5_col1 {\n",
       "  background-color: #91b1ca;\n",
       "  color: #000000;\n",
       "  background-color: #9a379a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e5f7a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e5f7a_level0_col0\" class=\"col_heading level0 col0\" >aopc_compr</th>\n",
       "      <th id=\"T_e5f7a_level0_col1\" class=\"col_heading level0 col1\" >aopc_suff</th>\n",
       "      <th id=\"T_e5f7a_level0_col2\" class=\"col_heading level0 col2\" >taucorr_loo</th>\n",
       "      <th id=\"T_e5f7a_level0_col3\" class=\"col_heading level0 col3\" >auprc_plau</th>\n",
       "      <th id=\"T_e5f7a_level0_col4\" class=\"col_heading level0 col4\" >token_f1_plau</th>\n",
       "      <th id=\"T_e5f7a_level0_col5\" class=\"col_heading level0 col5\" >token_iou_plau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f7a_level0_row0\" class=\"row_heading level0 row0\" >Partition SHAP</th>\n",
       "      <td id=\"T_e5f7a_row0_col0\" class=\"data row0 col0\" >0.05</td>\n",
       "      <td id=\"T_e5f7a_row0_col1\" class=\"data row0 col1\" >-0.47</td>\n",
       "      <td id=\"T_e5f7a_row0_col2\" class=\"data row0 col2\" >0.12</td>\n",
       "      <td id=\"T_e5f7a_row0_col3\" class=\"data row0 col3\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row0_col4\" class=\"data row0 col4\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row0_col5\" class=\"data row0 col5\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f7a_level0_row1\" class=\"row_heading level0 row1\" >LIME</th>\n",
       "      <td id=\"T_e5f7a_row1_col0\" class=\"data row1 col0\" >0.05</td>\n",
       "      <td id=\"T_e5f7a_row1_col1\" class=\"data row1 col1\" >-0.42</td>\n",
       "      <td id=\"T_e5f7a_row1_col2\" class=\"data row1 col2\" >0.55</td>\n",
       "      <td id=\"T_e5f7a_row1_col3\" class=\"data row1 col3\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row1_col4\" class=\"data row1 col4\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row1_col5\" class=\"data row1 col5\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f7a_level0_row2\" class=\"row_heading level0 row2\" >Gradient</th>\n",
       "      <td id=\"T_e5f7a_row2_col0\" class=\"data row2 col0\" >-0.05</td>\n",
       "      <td id=\"T_e5f7a_row2_col1\" class=\"data row2 col1\" >-0.26</td>\n",
       "      <td id=\"T_e5f7a_row2_col2\" class=\"data row2 col2\" >0.24</td>\n",
       "      <td id=\"T_e5f7a_row2_col3\" class=\"data row2 col3\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row2_col4\" class=\"data row2 col4\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row2_col5\" class=\"data row2 col5\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f7a_level0_row3\" class=\"row_heading level0 row3\" >Gradient (x Input)</th>\n",
       "      <td id=\"T_e5f7a_row3_col0\" class=\"data row3 col0\" >-0.01</td>\n",
       "      <td id=\"T_e5f7a_row3_col1\" class=\"data row3 col1\" >-0.18</td>\n",
       "      <td id=\"T_e5f7a_row3_col2\" class=\"data row3 col2\" >0.27</td>\n",
       "      <td id=\"T_e5f7a_row3_col3\" class=\"data row3 col3\" >0.04</td>\n",
       "      <td id=\"T_e5f7a_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
       "      <td id=\"T_e5f7a_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f7a_level0_row4\" class=\"row_heading level0 row4\" >Integrated Gradient</th>\n",
       "      <td id=\"T_e5f7a_row4_col0\" class=\"data row4 col0\" >0.01</td>\n",
       "      <td id=\"T_e5f7a_row4_col1\" class=\"data row4 col1\" >-0.35</td>\n",
       "      <td id=\"T_e5f7a_row4_col2\" class=\"data row4 col2\" >0.00</td>\n",
       "      <td id=\"T_e5f7a_row4_col3\" class=\"data row4 col3\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row4_col4\" class=\"data row4 col4\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row4_col5\" class=\"data row4 col5\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f7a_level0_row5\" class=\"row_heading level0 row5\" >Integrated Gradient (x Input)</th>\n",
       "      <td id=\"T_e5f7a_row5_col0\" class=\"data row5 col0\" >0.05</td>\n",
       "      <td id=\"T_e5f7a_row5_col1\" class=\"data row5 col1\" >-0.54</td>\n",
       "      <td id=\"T_e5f7a_row5_col2\" class=\"data row5 col2\" >0.27</td>\n",
       "      <td id=\"T_e5f7a_row5_col3\" class=\"data row5 col3\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row5_col4\" class=\"data row5 col4\" >1.00</td>\n",
       "      <td id=\"T_e5f7a_row5_col5\" class=\"data row5 col5\" >1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f17d09b9550>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation_evaluations = bench.evaluate_explanations(\n",
    "    all_explanations,\n",
    "    target=0,\n",
    "    # Negative words = \"sad\"\n",
    "    human_rationale=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    top_k_rationale = 1\n",
    ")\n",
    "bench.show_evaluation_table(explanation_evaluations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[‚û°Ô∏è Faithfulness and plausibility metrics explained](https://ferret.readthedocs.io/en/latest/user_guide/notions.benchmarking.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributing Generative Language Models with Inseq\n",
    "\n",
    "**Plan**:\n",
    "\n",
    "- Intro\n",
    "- Basic example + step scores\n",
    "- Subword + head/layer aggregation example\n",
    "- Minimal pair difference example\n",
    "- Contrastive feature attribution example\n",
    "- Localizing factual knowledge with layer-wise gradient attribution "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
